<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Applications of SVD | Divij Khaitan </title> <meta name="author" content="Divij Khaitan"> <meta name="description" content="Implementing applications of SVD"> <meta name="keywords" content="Mathematics, Computer Science, Computational Mathematics, Machine Learning, Artifical Intelligence, Deep Learning, Neural Networks, Multi Armed Bandits"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://divijkhaitan.github.io/projects/SVD_applications/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Divij</span> Khaitan </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Applications of SVD</h1> <p class="post-description">Implementing applications of SVD</p> </header> <article> <h2 id="singular-value-decomposition-introduction">Singular Value Decomposition: Introduction</h2> <p>Singular value decomposition is a matrix factorisation method, that expressed an arbitrary matrix \(A_{m \times n}\) as a product of 3 matrices, a nonnegative diagonal matrix \(\Sigma_{m \times n}\) and two orthogonal matrices \(U_{m \times m}\) and \(V^T_{n \times n}\), which can be written as</p> \[A = U\Sigma V^T\] <p>The entries of the diagonal matrix \(\Sigma\) are called the <strong>singular values</strong> of A, denoted as \(\sigma_i(A)\). For a symmetric matrix, \(\vert\lambda_i(A)\vert=\sigma_i(A)\), where \(\lambda_i\) is the \(i\)th largest eigenvalue of \(A\).</p> <p>In the complex domain, \(\Sigma\) is still nonnegative and real, \(U\) is unitary and \(V^*\) is the conjugate transpose of a unitary matrix \(V\).</p> <p>The number of nonzero entries in \(\Sigma\) equals the rank of \(A\). For a matrix \(A\) with rank \(r\), \(\Sigma\) has \(r\) nonzero diagonal entries, with the rest being zeros. It would look something like this</p> \[A = U \begin{bmatrix} \sigma_1 &amp; \dots &amp; 0 &amp; \dots &amp; 0 \\ \vdots &amp; \ddots &amp; \dots &amp; \ddots &amp; \vdots \\ 0 &amp; \dots &amp; \sigma_r &amp; \dots &amp; 0 \\ \vdots &amp; \ddots &amp; \dots &amp; \ddots &amp; \vdots \\ 0 &amp; \dots &amp; 0 &amp; \dots &amp; 0 \end{bmatrix} V^T\] <p>The fact that the rank equals the number of diagonal entries can be used to find bases for important subspaces related to the matrix \(A\). The first r columns of \(U\) form a basis for the column space of \(A\), and the last \(n - r\) columns of \(V\) are a basis for the null space of \(A\). Similarly, the first r columns of \(V\) are a basis for the column space of \(A^T\) which is also the row space of \(A\), and the last \(m -r\) columns of \(U\) form a basis for the null space of \(A^T\), which is also the orthogonal complement of the row space \(\mathbb{R}(A)^\perp\).</p> <h2 id="application-1-lsi">Application 1: LSI</h2> <p>Latent Semantic Indexing is a technique for information retrival that can be used to search for documents that are similar to a querying phrase. It is based on the principal that there exists a low dimension substructure to the space of documents.</p> <p>The first step is creating what is called a document-term matrix, where each row represents a document and each column denotes a term. There are several possible ways to fill up each column, including:</p> <ul> <li>Binary: \(a_{ij}\) = \(\mathbb{I}\{t_i \in d_j\}\)</li> <li>Frequency: \(a_{ij}\) = \(\#(t_i \in d_j)\)</li> <li>Inverse Frequency \(a_{ij}\) = \(\frac{c}{\#(t_i \in d_j)}\)</li> </ul> <p>Once the document term matrix is created, the next step is to compute a singular value decomposition. Now, assuming a high signal to noise ratio, the smaller singular values can be interpreted as noise and subsequently discarded. A rule of thumb for this I used was when \(\frac{\sigma{_i}}{\sigma_{i+1}} &gt; 10^c\) for some \(c &gt;= 3\). This should provide a significant rank reduction while preserving the structure of the document space.</p> <p>Queries can now be computed as follows. The SVD equation is \(A = U\Sigma'V^T\), where \(U\) is in the term space. \(V\) is in the document space and \(\Sigma'\) is the low rank approximation of \(\Sigma\). Each query vector is a member of the document space, so to compute the best matches we can rearrange the equation to \(V = A'U(\Sigma')\^{-1}\). To obtain the most similar documents, we just multiply the column vector of the query by \(U(\Sigma')\^{-1}\).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sv-480.webp 480w,/assets/img/sv-800.webp 800w,/assets/img/sv-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/sv.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="lsi_svd" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/qr-480.webp 480w,/assets/img/qr-800.webp 800w,/assets/img/qr-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/qr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="qr_svd" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>This isn’t simply a property of matrix decomposition, but one unique to SVD. Attached is a side by side comparison of the first two singular vectors and the analogous projection from the QR factorisation. While the SVD plots have some differentiation, and even a few clusters with similar books, the QR factorisation maps most documents to the same points and isn’t able to give any useful information.</p> <h4 id="polysemy-and-cosine-similarity">Polysemy and Cosine Similarity</h4> <p>Polysemy is the existence of multiple meanings for a word or phrase. If I have a dataset of textbooks from Mathematics and Computer Science, the word ‘function’ means a very different thing in the world of algorithm design compared to what it means in complex analysis. A natural concern is that queries which such words might lead to spurious matches. But that is where the dimensionality reduction from SVD is so useful. The latent space consists of ‘metawords’, so the co-ocurrence of ‘function’ with other terms from both fields would be captured by the metaword it corresponds to. This also extends to the inverse case, where words may be used interchangably, and different documents use one but not the other.</p> <p>Additionaly, when trying to retrieve information one might want to see which terms or documents are similar to one another. This can be done by analysing the reconstructed low-rank approximation, specifically \(X = A'(A')^T\) or \((A')^TA'\) respectively. \(X_{ij}\) will denote the similarity of the \(i^{th}\) term/document to the \(j^{th}\).</p> <h2 id="application-2-eigenfaces">Application 2: Eigenfaces</h2> <p>Eigenfaces was discovered in 1987 by Sirovich and Kirby. This is built on a similar idea of the space of all human faces having a low dimensional substructure. This algorithm performs a principal component analysis of faces, and projects every subsequent face onto the principal components, and differentiates faces based on their distance from the clusters.</p> <p>To begin with, I collected and cleaned a private dataset of around 200 images taken from about 15-20 of my friends. I ensured that the lighting and background were normalised across all of these images, else the directions of largest variance would be the backgrounds which would hamper the algorithm. For similar reasons, I ensured that the faces were completely visible in each image.</p> <p>The input images were flattened and organised into a matrix with each image being one row. I then computed the principal components for the matrix. One challenge I encountered here was that I had a wide matrix (many more features than examples). Computing a PCA using \(A^TA\) would have been quite expensive. The faster method is to compute \(AA^T\) and decompose it, which gives you one set of singular vectors for \(A\). The others can now be obtained by mutliplying this matrix of singular vectors with A.</p> <p>Having computed the principal components, the next step is projecting the training set onto them. A representative image for each class is computed by taking the mean of all the projections for that class. 2 parameters are tuned, one being for the threshold distance for an unknown face, and the second being a threshold for an image not being a face.</p> <p>The accuracy of the algorithm plateaus at around 6 principal components, suggesting that even just six components can cover a space of images from 15 different people extremely well.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/eigenfaces_accuracy-480.webp 480w,/assets/img/eigenfaces_accuracy-800.webp 800w,/assets/img/eigenfaces_accuracy-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/eigenfaces_accuracy.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="acc_plot" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Divij Khaitan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?5d75c11f89cd96294bf5e6dd1ee1bb30"></script> <script defer src="/assets/js/common.js?fcfacfb8c6281f5e68d5a7d348186eb1"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>